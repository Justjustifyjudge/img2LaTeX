{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7143799,"sourceType":"datasetVersion","datasetId":4123485},{"sourceId":7166466,"sourceType":"datasetVersion","datasetId":4131784}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-10T01:39:55.175342Z","iopub.execute_input":"2023-12-10T01:39:55.175816Z","iopub.status.idle":"2023-12-10T01:41:12.427565Z","shell.execute_reply.started":"2023-12-10T01:39:55.175788Z","shell.execute_reply":"2023-12-10T01:41:12.426683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 基本环境的准备","metadata":{}},{"cell_type":"code","source":"!pip install streamlit\n!pip install hydra-core\n!pip install python-multipart\n!pip install editdistance","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:41:12.429350Z","iopub.execute_input":"2023-12-10T01:41:12.430101Z","iopub.status.idle":"2023-12-10T01:42:06.124681Z","shell.execute_reply.started":"2023-12-10T01:41:12.430063Z","shell.execute_reply":"2023-12-10T01:42:06.123549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport json\nimport tarfile\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\nfrom urllib.request import urlretrieve\nimport pytorch_lightning.tuner as tuner\nfrom pytorch_lightning.tuner.lr_finder import _LRFinder\n\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\n# albumentations 是一个适用于计算机视觉任务的图像增强库\nimport albumentations as A\nimport torch\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader\n\nfrom argparse import Namespace\nfrom typing import List, Optional\n\nimport math\nimport torch.nn as nn\nfrom torch import Tensor\nfrom typing import Union\nimport torchvision.models\nfrom pytorch_lightning import LightningModule\nfrom typing import Set\nimport editdistance\nfrom torchmetrics import Metric\n\nimport hydra\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom pytorch_lightning.loggers.wandb import WandbLogger\n# import os\n# os.environ['CUDA_VISIBLE_DEVICES']='0'","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:42:06.126295Z","iopub.execute_input":"2023-12-10T01:42:06.126609Z","iopub.status.idle":"2023-12-10T01:42:12.290474Z","shell.execute_reply.started":"2023-12-10T01:42:06.126578Z","shell.execute_reply":"2023-12-10T01:42:12.289735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 运行时使用的一些工具函数，放在utils.py中","metadata":{}},{"cell_type":"code","source":"# utils.py\n\nclass TqdmUpTo(tqdm):\n    \"\"\"\n    From https://github.com/tqdm/tqdm/blob/master/examples/tqdm_wget.py.\n    继承自tqdm库中的tqdm类，用于追踪下载进度，并更新tqdm进度条\n    \"\"\"\n\n    def update_to(self, blocks=1, bsize=1, tsize=None) -> None:\n        \"\"\"Inform the progress bar how many data have been downloaded.\n\n        Args:\n            blocks: Number of blocks transferred so far.\n            bsize: Size of each block (in tqdm units).\n            tsize: Total size (in tqdm units).\n        \"\"\"\n        if tsize is not None:\n            self.total = tsize\n        self.update(blocks * bsize - self.n)\n\n\ndef download_url(url: str, filename: str) -> None:\n    \"\"\"Download a file from url to filename, with a progress bar.\"\"\"\n    with TqdmUpTo(unit=\"B\", unit_scale=True, unit_divisor=1024, miniters=1) as t:\n        t.set_description(filename)\n        urlretrieve(url, filename, reporthook=t.update_to, data=None)\n\n\ndef extract_tar_file(filename: str) -> None:\n    \"\"\"Extract a .tar or .tar.gz file.\"\"\"\n    print(f\"Extracting {filename}...\")\n    with tarfile.open(filename, \"r\") as f:\n        f.extractall()\n\n\nclass BaseDataset(Dataset):\n    \"\"\"\n    A base Dataset class.\n    继承自 torch.utils.data.Dataset 类，用于构建自定义的数据集类\n    Args:\n        image_filenames: (N, *) feature vector.\n        targets: (N, *) target vector relative to data.\n        transform: Feature transformation.\n        target_transform: Target transformation.\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir: Path,\n        image_filenames: List[str],\n        formulas: List[List[str]],\n        transform: Optional[Callable] = None,\n    ) -> None:\n        super().__init__()\n        assert len(image_filenames) == len(formulas)\n        self.root_dir = root_dir\n        self.image_filenames = image_filenames\n        self.formulas = formulas\n        self.transform = transform\n\n    def __len__(self) -> int:\n        \"\"\"Returns the number of samples.\"\"\"\n        return len(self.formulas)\n\n    def __getitem__(self, idx: int):\n        \"\"\"Returns a sample from the dataset at the given index.\"\"\"\n        image_filename, formula = self.image_filenames[idx], self.formulas[idx]\n        image_filepath = self.root_dir / image_filename\n        if image_filepath.is_file():\n            image = pil_loader(image_filepath, mode=\"L\")\n        else:\n            # Returns a blank image if cannot find the image\n            image = Image.fromarray(np.full((64, 128), 255, dtype=np.uint8))\n            formula = []\n        if self.transform is not None:\n            image = self.transform(image=np.array(image))[\"image\"]\n        return image, formula\n\n\ndef pil_loader(fp: Path, mode: str) -> Image.Image:\n    \"\"\"\n    用于加载图像文件的函数，使用PIL库来打开和处理图像文件，\n    将图像文件加载为 PIL 图像对象，并进行指定的模式转换后返回。\n    \"\"\"\n    with open(fp, \"rb\") as f:\n        img = Image.open(f)\n        return img.convert(mode)\n\n\nclass Tokenizer:\n    \"\"\"\n    用于处理文本数据的标记化(tokenizer)操作，将文本数据转换为数字表示。\n    可以根据提供的标记到索引字典进行索引和反索引的转换。\n    定义了一组特殊标记，以及对应的索引值，并且将这些特殊标记的索引添加到了 self.ignore_indices 集合中，以便在后续处理过程中进行忽略。\n    \"\"\"\n    def __init__(self, token_to_index: Optional[Dict[str, int]] = None) -> None:\n        self.pad_token = \"<PAD>\"\n        self.sos_token = \"<SOS>\"\n        self.eos_token = \"<EOS>\"\n        self.unk_token = \"<UNK>\"\n\n        self.token_to_index: Dict[str, int]\n        self.index_to_token: Dict[int, str]\n\n        if token_to_index:\n            self.token_to_index = token_to_index\n            self.index_to_token = {index: token for token, index in self.token_to_index.items()}\n            self.pad_index = self.token_to_index[self.pad_token]\n            self.sos_index = self.token_to_index[self.sos_token]\n            self.eos_index = self.token_to_index[self.eos_token]\n            self.unk_index = self.token_to_index[self.unk_token]\n        else:\n            self.token_to_index = {}\n            self.index_to_token = {}\n            self.pad_index = self._add_token(self.pad_token)\n            self.sos_index = self._add_token(self.sos_token)\n            self.eos_index = self._add_token(self.eos_token)\n            self.unk_index = self._add_token(self.unk_token)\n\n        self.ignore_indices = {self.pad_index, self.sos_index, self.eos_index, self.unk_index}\n\n    def _add_token(self, token: str) -> int:\n        \"\"\"Add one token to the vocabulary.\n\n        Args:\n            token: The token to be added.\n\n        Returns:\n            The index of the input token.\n        \"\"\"\n        if token in self.token_to_index:\n            return self.token_to_index[token]\n        index = len(self)\n        self.token_to_index[token] = index\n        self.index_to_token[index] = token\n        return index\n\n    def __len__(self):\n        return len(self.token_to_index)\n\n    def train(self, formulas: List[List[str]], min_count: int = 2) -> None:\n        \"\"\"Create a mapping from tokens to indices and vice versa.\n\n        Args:\n            formulas: Lists of tokens.\n            min_count: Tokens that appear fewer than `min_count` will not be\n                included in the mapping.\n        \"\"\"\n        # Count the frequency of each token\n        counter: Dict[str, int] = {}\n        for formula in formulas:\n            for token in formula:\n                counter[token] = counter.get(token, 0) + 1\n\n        for token, count in counter.items():\n            # Remove tokens that show up fewer than `min_count` times\n            if count < min_count:\n                continue\n            index = len(self)\n            self.index_to_token[index] = token\n            self.token_to_index[token] = index\n\n    def encode(self, formula: List[str]) -> List[int]:\n        indices = [self.sos_index]\n        for token in formula:\n            index = self.token_to_index.get(token, self.unk_index)\n            indices.append(index)\n        indices.append(self.eos_index)\n        return indices\n\n    def decode(self, indices: List[int], inference: bool = True) -> List[str]:\n        tokens = []\n        for index in indices:\n            if index not in self.index_to_token:\n                raise RuntimeError(f\"Found an unknown index {index}\")\n            if index == self.eos_index:\n                break\n            if inference and index in self.ignore_indices:\n                continue\n            token = self.index_to_token[index]\n            tokens.append(token)\n        return tokens\n\n    def save(self, filename: Union[Path, str]):\n        \"\"\"Save token-to-index mapping to a json file.\"\"\"\n        with open(filename, \"w\") as f:\n            json.dump(self.token_to_index, f)\n\n    @classmethod\n    def load(cls, filename: Union[Path, str]) -> \"Tokenizer\":\n        \"\"\"Create a `Tokenizer` from a mapping file outputted by `save`.\n\n        Args:\n            filename: Path to the file to read from.\n\n        Returns:\n            A `Tokenizer` object.\n        \"\"\"\n        with open(filename) as f:\n            token_to_index = json.load(f)\n        return cls(token_to_index)\n\n\ndef get_all_formulas(filename: Path) -> List[List[str]]:\n    \"\"\"Returns all the formulas in the formula file.\"\"\"\n    with open(filename, encoding='utf-8') as f:\n        all_formulas = [formula.strip(\"\\n\").split() for formula in f.readlines()]\n    return all_formulas\n\n\ndef get_split(\n    all_formulas: List[List[str]],\n    filename: Path,\n) -> Tuple[List[str], List[List[str]]]:\n    image_names = []\n    formulas = []\n    with open(filename) as f:\n        for line in f:\n            img_name, formula_idx = line.strip(\"\\n\").split()\n            image_names.append(img_name)\n            formulas.append(all_formulas[int(formula_idx)])\n    return image_names, formulas\n\n\ndef first_and_last_nonzeros(arr):\n    for i in range(len(arr)):\n        if arr[i] != 0:\n            break\n    left = i\n    for i in reversed(range(len(arr))):\n        if arr[i] != 0:\n            break\n    right = i\n    return left, right\n\n\ndef crop(filename: Path, padding: int = 8) -> Optional[Image.Image]:\n    \"\"\"\n    用于对图像进行裁剪和填充操作。\n    \"\"\"\n    image = pil_loader(filename, mode=\"RGBA\")\n\n    # Replace the transparency layer with a white background\n    new_image = Image.new(\"RGBA\", image.size, \"WHITE\")\n    new_image.paste(image, (0, 0), image)\n    new_image = new_image.convert(\"L\")\n\n    # Invert the color to have a black background and white text\n    arr = 255 - np.array(new_image)\n\n    # Area that has text should have nonzero pixel values\n    row_sums = np.sum(arr, axis=1)\n    col_sums = np.sum(arr, axis=0)\n    y_start, y_end = first_and_last_nonzeros(row_sums)\n    x_start, x_end = first_and_last_nonzeros(col_sums)\n\n    # Some images have no text\n    if y_start >= y_end or x_start >= x_end:\n        print(f\"{filename.name} is ignored because it does not contain any text\")\n        return None\n\n    # Cropping\n    cropped = arr[y_start : y_end + 1, x_start : x_end + 1]\n    H, W = cropped.shape\n\n    # Add paddings\n    new_arr = np.zeros((H + padding * 2, W + padding * 2))\n    new_arr[padding : H + padding, padding : W + padding] = cropped\n\n    # Invert the color back to have a white background and black text\n    new_arr = 255 - new_arr\n    return Image.fromarray(new_arr).convert(\"L\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:42:12.292764Z","iopub.execute_input":"2023-12-10T01:42:12.293171Z","iopub.status.idle":"2023-12-10T01:42:12.333080Z","shell.execute_reply.started":"2023-12-10T01:42:12.293146Z","shell.execute_reply":"2023-12-10T01:42:12.332078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 把处理IM2LaTeX数据集的py文件搬运到了im2latex.py中，在这里设置了batch_size等超参数和训练时的dataloader","metadata":{}},{"cell_type":"code","source":"#im2latex.py\n\nclass Im2Latex(LightningDataModule):\n    \"\"\"Data processing for the Im2Latex-100K dataset.\n\n    Args:\n        batch_size: The number of samples per batch.\n        num_workers: The number of subprocesses to use for data loading.\n        pin_memory: If True, the data loader will copy Tensors into CUDA pinned memory\n            before returning them.\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ) -> None:\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.pin_memory = pin_memory\n\n        # self.data_dirname = Path(__file__).resolve().parents[2] / \"data\"\n        self.vocab_file = Path(\"/kaggle/input/dataset1/lst\") / \"vocab.json\"\n        # formula_file = self.data_dirname / \"im2latex_formulas.norm.new.lst\"\n        formula_file = Path(\"/kaggle/input/dataset1/lst\") / \"im2latex_formulas.norm.new.lst\"\n        # if not formula_file.is_file():\n        #     raise FileNotFoundError(\"Did you run scripts/prepare_data.py?\")\n        self.all_formulas = get_all_formulas(formula_file)\n        self.transform = {\n            \"train\": A.Compose(\n                [\n                    A.Affine(scale=(0.6, 1.0), rotate=(-1, 1), cval=255, p=0.5),\n                    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n                    A.GaussianBlur(blur_limit=(1, 1), p=0.5),\n                    ToTensorV2(),\n                ]\n            ),\n            \"val/test\": ToTensorV2(),\n        }\n\n    # @property\n    # def processed_images_dirname(self):\n    #     return self.data_dirname / \"formula_images_processed\"\n\n    def setup(self, stage: Optional[str] = None) -> None:\n        \"\"\"Load images and formulas, and assign them to a `torch Dataset`.\n\n        `self.train_dataset`, `self.val_dataset` and `self.test_dataset` will\n        be assigned after this method is called.\n        \"\"\"\n        self.tokenizer = Tokenizer.load(self.vocab_file)\n\n        if stage in (\"fit\", None):\n            train_image_names, train_formulas = get_split(\n                self.all_formulas,\n                # self.data_dirname / \"im2latex_train_filter.lst\",\n                Path(\"/kaggle/input/dataset1/lst\") / \"im2latex_train_filter.lst\",\n            )\n            self.train_dataset = BaseDataset(\n                # self.processed_images_dirname,\n                Path(\"/kaggle/input/dataset1/images_3/train\"),\n                image_filenames=train_image_names,\n                formulas=train_formulas,\n                transform=self.transform[\"train\"],\n            )\n\n            val_image_names, val_formulas = get_split(\n                self.all_formulas,\n                # self.data_dirname / \"im2latex_validate_filter.lst\",\n                Path(\"/kaggle/input/dataset1/lst\") / \"im2latex_validate_filter.lst\",\n            )\n            self.val_dataset = BaseDataset(\n                # self.processed_images_dirname,\n                Path(\"/kaggle/input/dataset1/images_3/val\"),\n                image_filenames=val_image_names,\n                formulas=val_formulas,\n                transform=self.transform[\"val/test\"],\n            )\n\n        if stage in (\"test\", None):\n            test_image_names, test_formulas = get_split(\n                self.all_formulas,\n                # self.data_dirname / \"im2latex_test_filter.lst\",\n                Path(\"/kaggle/input/dataset1/lst\") / \"im2latex_test_filter.lst\",\n            )\n            self.test_dataset = BaseDataset(\n                # self.processed_images_dirname,\n                Path(\"/kaggle/input/dataset1/images_3/test\"),\n                image_filenames=test_image_names,\n                formulas=test_formulas,\n                transform=self.transform[\"val/test\"],\n            )\n\n    def collate_fn(self, batch):\n        images, formulas = zip(*batch)\n        B = len(images)\n        max_H = max(image.shape[1] for image in images)\n        max_W = max(image.shape[2] for image in images)\n        max_length = max(len(formula) for formula in formulas)\n        padded_images = torch.zeros((B, 1, max_H, max_W))\n        batched_indices = torch.zeros((B, max_length + 2), dtype=torch.long)\n        for i in range(B):\n            H, W = images[i].shape[1], images[i].shape[2]\n            y, x = random.randint(0, max_H - H), random.randint(0, max_W - W)\n            padded_images[i, :, y : y + H, x : x + W] = images[i]\n            indices = self.tokenizer.encode(formulas[i])\n            batched_indices[i, : len(indices)] = torch.tensor(indices, dtype=torch.long)\n        return padded_images, batched_indices\n\n    def train_dataloader(self) -> DataLoader:\n        return DataLoader(\n            self.train_dataset,\n            shuffle=True,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            collate_fn=self.collate_fn,\n        )\n\n    def val_dataloader(self) -> DataLoader:\n        return DataLoader(\n            self.val_dataset,\n            shuffle=False,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            collate_fn=self.collate_fn,\n        )\n\n    def test_dataloader(self) -> DataLoader:\n        return DataLoader(\n            self.test_dataset,\n            shuffle=False,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            collate_fn=self.collate_fn,\n        )","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:42:12.334699Z","iopub.execute_input":"2023-12-10T01:42:12.334994Z","iopub.status.idle":"2023-12-10T01:42:12.355729Z","shell.execute_reply.started":"2023-12-10T01:42:12.334971Z","shell.execute_reply":"2023-12-10T01:42:12.354903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 位置编码","metadata":{}},{"cell_type":"code","source":"# positional_encoding.py\n\nclass PositionalEncoding2D(nn.Module):\n    \"\"\"2-D positional encodings for the feature maps produced by the encoder.\n\n    Following https://arxiv.org/abs/2103.06450 by Sumeet Singh.\n\n    Reference:\n    https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2021-labs/blob/main/lab9/text_recognizer/models/transformer_util.py\n    \"\"\"\n\n    def __init__(self, d_model: int, max_h: int = 2000, max_w: int = 2000) -> None:\n        super().__init__()\n        self.d_model = d_model\n        assert d_model % 2 == 0, f\"Embedding depth {d_model} is not even\"\n        pe = self.make_pe(d_model, max_h, max_w)  # (d_model, max_h, max_w)\n        self.register_buffer(\"pe\", pe)\n\n    @staticmethod\n    def make_pe(d_model: int, max_h: int, max_w: int) -> Tensor:\n        \"\"\"Compute positional encoding.\"\"\"\n        pe_h = PositionalEncoding1D.make_pe(d_model=d_model // 2, max_len=max_h)  # (max_h, 1 d_model // 2)\n        pe_h = pe_h.permute(2, 0, 1).expand(-1, -1, max_w)  # (d_model // 2, max_h, max_w)\n\n        pe_w = PositionalEncoding1D.make_pe(d_model=d_model // 2, max_len=max_w)  # (max_w, 1, d_model // 2)\n        pe_w = pe_w.permute(2, 1, 0).expand(-1, max_h, -1)  # (d_model // 2, max_h, max_w)\n\n        pe = torch.cat([pe_h, pe_w], dim=0)  # (d_model, max_h, max_w)\n        return pe\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass.\n\n        Args:\n            x: (B, d_model, H, W)\n\n        Returns:\n            (B, d_model, H, W)\n        \"\"\"\n        assert x.shape[1] == self.pe.shape[0]  # type: ignore\n        x = x + self.pe[:, : x.size(2), : x.size(3)]  # type: ignore\n        return x\n\n\nclass PositionalEncoding1D(nn.Module):\n    \"\"\"Classic Attention-is-all-you-need positional encoding.\"\"\"\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = self.make_pe(d_model, max_len)  # (max_len, 1, d_model)\n        self.register_buffer(\"pe\", pe)\n\n    @staticmethod\n    def make_pe(d_model: int, max_len: int) -> Tensor:\n        \"\"\"Compute positional encoding.\"\"\"\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(1)\n        return pe\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"Forward pass.\n\n        Args:\n            x: (S, B, d_model)\n\n        Returns:\n            (B, d_model, H, W)\n        \"\"\"\n        assert x.shape[2] == self.pe.shape[2]  # type: ignore\n        x = x + self.pe[: x.size(0)]  # type: ignore\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:42:12.357056Z","iopub.execute_input":"2023-12-10T01:42:12.357313Z","iopub.status.idle":"2023-12-10T01:42:12.373772Z","shell.execute_reply.started":"2023-12-10T01:42:12.357290Z","shell.execute_reply":"2023-12-10T01:42:12.372968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ResNet&Transformer模型的定义","metadata":{}},{"cell_type":"code","source":"# resnet_transformer.py\n\nclass ResNetTransformer(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        dim_feedforward: int,\n        nhead: int,\n        dropout: float,\n        num_decoder_layers: int,\n        max_output_len: int,\n        sos_index: int,\n        eos_index: int,\n        pad_index: int,\n        num_classes: int,\n    ) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.max_output_len = max_output_len + 2\n        self.sos_index = sos_index\n        self.eos_index = eos_index\n        self.pad_index = pad_index\n\n        # Encoder\n        resnet = torchvision.models.resnet50(pretrained=False)\n        self.backbone = nn.Sequential(\n            resnet.conv1,\n            resnet.bn1,\n            resnet.relu,\n            resnet.maxpool,\n            resnet.layer1,\n            resnet.layer2,\n            resnet.layer3,\n            resnet.layer4,\n        )\n\n        self.bottleneck = nn.Conv2d(2048, self.d_model, 1)\n        self.image_positional_encoder = PositionalEncoding2D(self.d_model)\n\n        # Decoder\n        self.embedding = nn.Embedding(num_classes, self.d_model)\n        self.y_mask = generate_square_subsequent_mask(self.max_output_len)\n        self.word_positional_encoder = PositionalEncoding1D(self.d_model, max_len=self.max_output_len)\n        transformer_decoder_layer = nn.TransformerDecoderLayer(self.d_model, nhead, dim_feedforward, dropout)\n        self.transformer_decoder = nn.TransformerDecoder(transformer_decoder_layer, num_decoder_layers)\n        self.fc = nn.Linear(self.d_model, num_classes)\n\n        # It is empirically important to initialize weights properly\n        if self.training:\n            self._init_weights()\n\n    def _init_weights(self) -> None:\n        \"\"\"Initialize weights.\"\"\"\n        init_range = 0.1\n        self.embedding.weight.data.uniform_(-init_range, init_range)\n        self.fc.bias.data.zero_()\n        self.fc.weight.data.uniform_(-init_range, init_range)\n\n        nn.init.kaiming_normal_(\n            self.bottleneck.weight.data,\n            a=0,\n            mode=\"fan_out\",\n            nonlinearity=\"relu\",\n        )\n        if self.bottleneck.bias is not None:\n            _, fan_out = nn.init._calculate_fan_in_and_fan_out(self.bottleneck.weight.data)\n            bound = 1 / math.sqrt(fan_out)\n            nn.init.normal_(self.bottleneck.bias, -bound, bound)\n\n    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n        \"\"\"Forward pass.\n\n        Args:\n            x: (B, _E, _H, _W)\n            y: (B, Sy) with elements in (0, num_classes - 1)\n\n        Returns:\n            (B, num_classes, Sy) logits\n        \"\"\"\n        encoded_x = self.encode(x)  # (Sx, B, E)\n        output = self.decode(y, encoded_x)  # (Sy, B, num_classes)\n        output = output.permute(1, 2, 0)  # (B, num_classes, Sy)\n        return output\n\n    def encode(self, x: Tensor) -> Tensor:\n        \"\"\"Encode inputs.\n\n        Args:\n            x: (B, C, _H, _W)\n\n        Returns:\n            (Sx, B, E)\n        \"\"\"\n        # Resnet expects 3 channels but training images are in gray scale\n        if x.shape[1] == 1:\n            x = x.repeat(1, 3, 1, 1)\n        x = self.backbone(x)  # (B, RESNET_DIM, H, W); H = _H // 32, W = _W // 32\n        x = self.bottleneck(x)  # (B, E, H, W)\n        x = self.image_positional_encoder(x)  # (B, E, H, W)\n        x = x.flatten(start_dim=2)  # (B, E, H * W)\n        x = x.permute(2, 0, 1)  # (Sx, B, E); Sx = H * W\n        return x\n\n    def decode(self, y: Tensor, encoded_x: Tensor) -> Tensor:\n        \"\"\"Decode encoded inputs with teacher-forcing.\n\n        Args:\n            encoded_x: (Sx, B, E)\n            y: (B, Sy) with elements in (0, num_classes - 1)\n\n        Returns:\n            (Sy, B, num_classes) logits\n        \"\"\"\n        y = y.permute(1, 0)  # (Sy, B)\n        y = self.embedding(y) * math.sqrt(self.d_model)  # (Sy, B, E)\n        y = self.word_positional_encoder(y)  # (Sy, B, E)\n        Sy = y.shape[0]\n        y_mask = self.y_mask[:Sy, :Sy].type_as(encoded_x)  # (Sy, Sy)\n        output = self.transformer_decoder(y, encoded_x, y_mask)  # (Sy, B, E)\n        output = self.fc(output)  # (Sy, B, num_classes)\n        return output\n\n    def predict(self, x: Tensor) -> Tensor:\n        \"\"\"Make predctions at inference time.\n\n        Args:\n            x: (B, C, H, W). Input images.\n\n        Returns:\n            (B, max_output_len) with elements in (0, num_classes - 1).\n        \"\"\"\n        B = x.shape[0]\n        S = self.max_output_len\n\n        encoded_x = self.encode(x)  # (Sx, B, E)\n\n        output_indices = torch.full((B, S), self.pad_index).type_as(x).long()\n        output_indices[:, 0] = self.sos_index\n        has_ended = torch.full((B,), False)\n\n        for Sy in range(1, S):\n            y = output_indices[:, :Sy]  # (B, Sy)\n            logits = self.decode(y, encoded_x)  # (Sy, B, num_classes)\n            # Select the token with the highest conditional probability\n            output = torch.argmax(logits, dim=-1)  # (Sy, B)\n            output_indices[:, Sy] = output[-1:]  # Set the last output token\n\n            # Early stopping of prediction loop to speed up prediction\n            has_ended |= (output_indices[:, Sy] == self.eos_index).type_as(has_ended)\n            if torch.all(has_ended):\n                break\n\n        # Set all tokens after end token to be padding\n        eos_positions = find_first(output_indices, self.eos_index)\n        for i in range(B):\n            j = int(eos_positions[i].item()) + 1\n            output_indices[i, j:] = self.pad_index\n\n        return output_indices\n\n\ndef generate_square_subsequent_mask(size: int) -> Tensor:\n    \"\"\"Generate a triangular (size, size) mask.\"\"\"\n    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef find_first(x: Tensor, element: Union[int, float], dim: int = 1) -> Tensor:\n    \"\"\"Find the first occurence of element in x along a given dimension.\n\n    Args:\n        x: The input tensor to be searched.\n        element: The number to look for.\n        dim: The dimension to reduce.\n\n    Returns:\n        Indices of the first occurence of the element in x. If not found, return the\n        length of x along dim.\n\n    Usage:\n        >>> first_element(Tensor([[1, 2, 3], [2, 3, 3], [1, 1, 1]]), 3)\n        tensor([2, 1, 3])\n\n    Reference:\n        https://discuss.pytorch.org/t/first-nonzero-index/24769/9\n\n        I fixed an edge case where the element we are looking for is at index 0. The\n        original algorithm will return the length of x instead of 0.\n    \"\"\"\n    mask = x == element\n    found, indices = ((mask.cumsum(dim) == 1) & mask).max(dim)\n    indices[(~found) & (indices == 0)] = x.shape[dim]\n    return indices","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:42:12.375198Z","iopub.execute_input":"2023-12-10T01:42:12.375583Z","iopub.status.idle":"2023-12-10T01:42:12.402952Z","shell.execute_reply.started":"2023-12-10T01:42:12.375552Z","shell.execute_reply":"2023-12-10T01:42:12.402116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 计算字符错误率","metadata":{}},{"cell_type":"code","source":"# metrics.py\n\nclass CharacterErrorRate(Metric):\n    \"\"\"\n    继承自torchmetrics中的Metric类 用于计算字符错误率\n    \"\"\"\n    def __init__(self, ignore_indices: Set[int], *args):\n        super().__init__(*args)\n        self.ignore_indices = ignore_indices\n        self.add_state(\"error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.error: Tensor\n        self.total: Tensor\n\n    def update(self, preds, targets):\n        N = preds.shape[0]\n        for i in range(N):\n            pred = [token for token in preds[i].tolist() if token not in self.ignore_indices]\n            target = [token for token in targets[i].tolist() if token not in self.ignore_indices]\n            distance = editdistance.distance(pred, target)\n            if max(len(pred), len(target)) > 0:\n                self.error += distance / max(len(pred), len(target))\n        self.total += N\n\n    def compute(self) -> Tensor:\n        return self.error / self.total","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:42:12.403839Z","iopub.execute_input":"2023-12-10T01:42:12.404092Z","iopub.status.idle":"2023-12-10T01:42:12.416472Z","shell.execute_reply.started":"2023-12-10T01:42:12.404070Z","shell.execute_reply":"2023-12-10T01:42:12.415808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 这是对模型、损失函数、优化器的合体的封装。里面放了train_step和val_step两个函数分别用来计算训练损失和验证损失，用test_step完成对测试集的训练。\n### on_test_epoch_end函数用于训练结束后写入文件。","metadata":{}},{"cell_type":"code","source":"# lit_resnet_transformer.py\n\nclass LitResNetTransformer(LightningModule):\n    def __init__(\n        self,\n        d_model: int,\n        dim_feedforward: int,\n        nhead: int,\n        dropout: float,\n        num_decoder_layers: int,\n        max_output_len: int,\n        lr: float = 0.001,\n        weight_decay: float = 0.0001,\n        milestones: List[int] = [5],\n        gamma: float = 0.1,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.milestones = milestones\n        self.gamma = gamma\n\n        vocab_file = \"/kaggle/input/dataset1/lst/vocab.json\"\n        self.tokenizer = Tokenizer.load(vocab_file)\n        self.model = ResNetTransformer(\n            d_model=d_model,\n            dim_feedforward=dim_feedforward,\n            nhead=nhead,\n            dropout=dropout,\n            num_decoder_layers=num_decoder_layers,\n            max_output_len=max_output_len,\n            sos_index=self.tokenizer.sos_index,\n            eos_index=self.tokenizer.eos_index,\n            pad_index=self.tokenizer.pad_index,\n            num_classes=len(self.tokenizer),\n        )\n        self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_index)\n        self.val_cer = CharacterErrorRate(self.tokenizer.ignore_indices)\n        self.test_cer = CharacterErrorRate(self.tokenizer.ignore_indices)\n\n        self.test_step_outputs = []\n        self.test_target = []\n\n    def training_step(self, batch, batch_idx):\n        imgs, targets = batch\n        logits = self.model(imgs, targets[:, :-1])\n        loss = self.loss_fn(logits, targets[:, 1:])\n        self.log(\"train/loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        imgs, targets = batch\n        logits = self.model(imgs, targets[:, :-1])\n        loss = self.loss_fn(logits, targets[:, 1:])\n        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        preds = self.model.predict(imgs)\n        val_cer = self.val_cer(preds, targets)\n        self.log(\"val/cer\", val_cer)\n\n    def test_step(self, batch, batch_idx):\n        imgs, targets = batch\n        self.test_target.append(targets)\n        preds = self.model.predict(imgs)\n        test_cer = self.test_cer(preds, targets)\n        self.log(\"test/cer\", test_cer)\n        self.test_step_outputs.append(preds)\n        return preds\n\n    # 在2.0.0中已经被替换\n    # def test_epoch_end(self, test_outputs):\n    #     with open(\"test_predictions.txt\", \"w\") as f:\n    #         for preds in test_outputs:\n    #             for pred in preds:\n    #                 decoded = self.tokenizer.decode(pred.tolist())\n    #                 decoded.append(\"\\n\")\n    #                 decoded_str = \" \".join(decoded)\n    #                 f.write(decoded_str)\n\n    def on_test_epoch_end(self):\n        with open(\"test_predictions.txt\", \"w\") as f:\n            for preds in self.test_step_outputs:\n                for pred in preds:\n                    decoded = self.tokenizer.decode(pred.tolist())\n                    decoded.append(\"\\n\")\n                    decoded_str = \" \".join(decoded)\n                    f.write(decoded_str)\n        with open(\"test_targets.txt\", \"w\") as f:\n            for targets in self.test_target:\n                for target in targets:\n                    decoded = self.tokenizer.decode(target.tolist())\n                    decoded.append(\"\\n\")\n                    decoded_str = \" \".join(decoded)\n                    f.write(decoded_str)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n        return [optimizer], [scheduler]","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:42:12.417618Z","iopub.execute_input":"2023-12-10T01:42:12.417897Z","iopub.status.idle":"2023-12-10T01:42:12.436018Z","shell.execute_reply.started":"2023-12-10T01:42:12.417874Z","shell.execute_reply":"2023-12-10T01:42:12.435226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 载入数据集完成训练","metadata":{}},{"cell_type":"code","source":"# 指定配置文件所在的目录\nhydra.initialize_config_dir(config_dir=\"/kaggle/input/config/\")\n\n# 加载并合并配置文件\ncfg = hydra.compose(config_name=\"config\")\n\n# print(cfg)\ndatamodule = Im2Latex(**cfg.data)\ndatamodule.setup()\n# print(cfg.data)\nlit_model = LitResNetTransformer(**cfg.lit_model)\n\ncallbacks: List[Callback] = []\nif cfg.callbacks.model_checkpoint:\n    callbacks.append(ModelCheckpoint(**cfg.callbacks.model_checkpoint, dirpath=Path(\"/kaggle/working/\")))\n    print(\"cfg.callbacks.model_checkpoint\")\nif cfg.callbacks.early_stopping:\n    callbacks.append(EarlyStopping(**cfg.callbacks.early_stopping))\n    print(\"cfg.callbacks.early_stopping\")\n\nlogger: Optional[WandbLogger] = None\nif cfg.logger:\n    logger = WandbLogger(**cfg.logger)\n    print(\"cfg.logger\")\n\ntrainer = Trainer(**cfg.trainer, callbacks=callbacks, logger=logger) # , \n#                   accelerator='gpu', devices=2, auto_select_gpus=True)\n\nif trainer.logger:\n    trainer.logger.log_hyperparams(Namespace(**cfg))\n\n# trainer.tune(lit_model, datamodule=datamodule)\n# temp = tuner.Tuner(trainer)\n# lr_best = temp.lr_find(lit_model, datamodule=datamodule)\n# print(\"lr is: \"+ lr_best.suggestion())\n# lit_model.lr = lr_best.suggestion()\n# optimal_batch_size = temp.scale_batch_size(lit_model, datamodule=datamodule)\n# datamodule.batch_size = int(optimal_batch_size/2)\n# datamodule.batch_size = 8\n\n\ntrainer.fit(lit_model, datamodule=datamodule)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T01:42:12.438287Z","iopub.execute_input":"2023-12-10T01:42:12.438557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 生成测试数据","metadata":{}},{"cell_type":"code","source":"trainer.test(lit_model, datamodule=datamodule, ckpt_path=\"/los=0.04/cer=0.02/checkpoint.ckpt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}